\documentclass[11pt]{hw-template}

\usepackage{enumitem}
%\usepackage{graphicx}
%\usepackage{mathtools}
%\lstset{showstringspaces=false}
\setlength\parindent{0pt}

\coursename{COMS 4995 Methods is Unsupervised Machine Learning (Fall 2018)}

\studname{Alexandre Lamy, Trung Vu, Sean Lee}
\studmail{$\set{\text{all2187, ttv2107, sl3794}}$@columbia.edu}
\assignmentName{Project Proposal}
\collab{}

\begin{document}
\maketitle

\Oldsection*{Background}

The seminal work done in \cite{NIPS2014_5423} proposes a novel architecture, GANs, for estimating generative models. In addition to describing the model and giving some experimental performances,
this work shows that the global optima in the space of arbitrary generative and discriminative functions is that where the generator perfectly learns the data distribution and the discriminator is
the naive 0.5 random guesser. However, this leaves unanswered two main more important questions. First, the results only apply when the discriminative and generative functions are arbitrarily powerful
(i.e. as stated in \cite{NIPS2014_5423} it applies only in the ''non-parametric limit''). What happens when we use depth and size limited neural networks instead? Second, this paper looks at the
theoretical global optima and gives no explanation as to what happens when the model is trained on finitely many samples. Some sort of generalization guarantee, i.e. how close the generator's distribution
comes to the true data distribution as a function of sample size, is dearly needed. 

\Oldsection*{Objective}

In short our objective will be to look at these two questions, i.e. \textit{what are the generalization guarantees of GANs when trained on finite samples}, and 
\textit{how does restricting the class of generative and/or descrimative functions
affect those guarantees}. We will seek first to understand the partial answers that currently exists to these questions and possibly to extend and improve upon those answers.

\Oldsection*{Recent advancements}

As mentioned, we will start by understanding the partial answers that some recent (2017 and 2018) papers have given to our two core questions. 
In particular there are five papers that we will look at to start our project.\\

The authors of \cite{NIPS2017_7240} consider the convergence of real life GANs training. As mentioned before, in practice it is impossible to have arbitrarily complex generative and discriminative functions. This means
that many local Nash equilibriums could exist. Furthermore, since GANs is formulated as a two player game in which the players have competing goals, techniques like gradient descent could fail to converge at all.
The authors of \cite{NIPS2017_7240} show that by using a two time scale update rule (TTUR), where the discriminator is updated more often than the generator, one can prove that the training of GANs
with TTUR and Adam will converge to
a stationary local Nash equilibrium. The authors also propose the ``Frechet Inception Distance'' (FID) as a new and more consistent way to evaluate GANs.\\

The authors of \cite{arora2017generalization} pose two questions. First, they tackle the finite sample case. How many examples do we need in order for the empirical distance between the
generated and true distributions to converge to the population distance, and
under what distance does convergence hold? The answer is that there is a notion of distance, defined as neural network distance, that allows convergence, but this notion
of distance is so weak that even when it is small, it is hard to conclude that the generated and true distributions are actually close.
Second, they tackle the assumption of infinite power in $\cite{NIPS2014_5423}$. They show that a generator with finite capacity can
still win against a discriminator with finite capacitor -- thus explaining why optimization halts in practice.\\

\cite{arora2018do} addresses an issue raised by \cite{arora2017generalization}: the neural network distance converges even
if the generated distribution is not close to the true distribution. Specifically, it might converge even if the generated
distribution has small support, leading to a phenomenon called mode collapse and a lack of diversity in the generated
examples. Thus it proposes a test based on the Birthday Paradox: if a distribution has small support then a random
batch will most likely contain duplicates.\\

The authors of \cite{zhang2018on} render GANs' performance
into a trade-off between the discriminative and generative power of the network, and provide theoretical guarantees on the discriminative powers
of the network in case the discriminators are dense enough span the set of bounded continuous functions; on the other hand, the generative powers
are guaranteed when the discriminator set is small enough. They provide a metric called \textit{neural distance} as a way to quantify such an upper bound.\\

The authors of \cite{liu2017approximation} elaborate on the convergence rate of GANs given characteristics of the objective function: if the objective function
satisfies \textit{strict adversarial divergence}, then there is a weak convergence of the GAN output distribution to the objective function.

\Oldsection*{Persisting issues}

As seen in the above, although our questions have been partially answered in various restricted settings, the convergence results are quite weak and generalization guarantees are almost non-existent.
In addition to getting an understanding of these partial results, we want to at least investigate why stronger results haven't been found, or in other words find out what causes the difficulty of this problem.


\bibliographystyle{ieeetr}
\bibliography{references}
\end{document}
