\documentclass[11pt]{hw-template}

\usepackage{enumitem}
%\usepackage{graphicx}
%\usepackage{mathtools}
%\lstset{showstringspaces=false}
\setlength\parindent{0pt}

\coursename{COMS 4995 Methods is Unsupervised Machine Learning (Fall 2018)}

\studname{Alexandre Lamy, Trung Vu, Sean Lee}
\studmail{$\set{\text{all2187, ttv2107, sl3794}}$@columbia.edu}
\assignmentName{Project Proposal}
\collab{}

\begin{document}
\maketitle

The seminal work done in \cite{NIPS2014_5423} proposes a novel architecture, GANs, for estimating generative models. In addition to describing the model and giving some experimental performances, 
this work shows that the global optima in the space of arbitrary generative and discriminative functions is that where the generator perfectly learns the data distribution and the discriminator is 
the naive 0.5 random guesser. However, this leaves unanswered two main more important questions. First, the results only apply when the discriminative and generative functions are arbitrarily powerful 
(i.e. as stated in \cite{NIPS2014_5423} it applies only in the ''non-parametric limit''). What happens when we use depth and size limited neural networks instead? Second, this paper looks at the 
theoretical global optima and gives no explanation as to what happens when the model is trained on finitely many samples. Some sort of generalization guarantee, i.e. how close the generator's distribution 
comes to the true data distribution as a function of sample size, is dearly needed. These questions have still not been completely and satisfactorily answered, although some more recent (2017 and 2018) papers
do try to address these issues. We will look at these newer results and try to extend them or at the very least present a complete picture of them.

The authors of \cite{NIPS2017_7240} consider the convergence of real life GANs training. As mentioned before, in practice it is impossible to have arbitrarily complex generative and discriminative. This means
that many local Nash equilibriums could exist. Furthermore, since GANs is formulated as a two player game in which the players have competing goals, techniques like gradient descent could fail to converge at all.
The authors of \cite{NIPS2017_7240} show that by using a two time scale update rule (TTUR), where the discriminator is updated more often than the generator, one can prove that the training of GANs 
with TTUR and Adam will converge to 
a stationary local Nash equilibrium. The authors also propose the ``Frechet Inception Distance'' (FID) as a new and more consistent way to evaluate GANs.

Work has been done to find provable guarantees to GANs under certain conditions. The authors of \cite{zhang2018on} render GANs' performance into a trade-off between the discriminative and generative power of the network, and provide theoretical guarantees on the discriminative powers of the network in case the discriminators are dense enough span the set of bounded continuous functions; on the other hand, the generative powers are guaranteed when the discriminator set is small enough. They provide a metric called \textit{neural distance} as a way to quantify such an upper bound. The authors of \cite{liu2017approximation} elaborates on the convergence rate of GANs given characteristics of the objective function: if the objective function satisfies \textit{strict adversarial divergence}, the there is a weak convergence of the GAN output distribution to the objective function.

\bibliographystyle{ieeetr}
\bibliography{references} 
\end{document} 
